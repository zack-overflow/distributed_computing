{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Assignment: Commercial Data Analysis\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS  \n",
    "In this assignment, you will work with a dataset containing information about businesses.  \n",
    "Each record is a business location.  Follow the steps below, writing and running the code in blocks, and displaying the solutions.  \n",
    "\n",
    "Each question part is worth 1 POINT, for a total of 15 POINTS.\n",
    "\n",
    "Hint: reaching deeper fields in json hierarchy can be done like this:  \n",
    "\n",
    "`df.select('address.street_number')`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 11:35:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/20 11:35:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/20 11:35:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/09/20 11:35:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/09/20 11:35:01 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note that read.json can read a zipped JSON directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (1 PT) Read in the dataset and show the number of records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 11:35:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "filename = \"/standard/ds7200-apt4c/large_datasets/part-00000-a159c41a-bc58-4476-9b78-c437667f9c2b-c000.json.gz\"\n",
    "df = spark.read.json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154679"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (1 PT) Show the first 5 records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------------+----+--------------------+--------------------+--------------------+\n",
      "|             address|       business_tags|               hours|              id|menu|             reviews|                urls|             webpage|\n",
      "+--------------------+--------------------+--------------------+----------------+----+--------------------+--------------------+--------------------+\n",
      "|{Woodburn, {45.15...|                null|                null|000023995a540868|null|                  []|{woodburn.k12.or....|{Educational Tech...|\n",
      "|{Hialeah, {25.884...|{[], [{has_atm, Y...|{null, 1900, null...|0000821a1394916e|null|                null|{null, [yelp.com]...|                null|\n",
      "|{Rochester, {43.1...|{[], [{accepts_cr...|{null, 1700, null...|000136e65d50c3b7|null|[{New (to me) qui...|{usps.com, [yelp....|{Welcome | USPS G...|\n",
      "|{West Palm Beach,...|                null|                null|00014329a70b9869|null|                null|                null|                null|\n",
      "|{Eufaula, {35.283...|                null|{null, 1700, null...|00031c0a83f00657|null|                null|{drsodomcoburnand...|{DRS. COBURN, RIC...|\n",
      "+--------------------+--------------------+--------------------+----------------+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (1 PT) Show the first 5 street addresses which are not null**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             address|\n",
      "+--------------------+\n",
      "|{Woodburn, {45.15...|\n",
      "|{Hialeah, {25.884...|\n",
      "|{Rochester, {43.1...|\n",
      "|{West Palm Beach,...|\n",
      "|{Eufaula, {35.283...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('address')).filter(col('address').isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (1 PT) Location**  \n",
    "\n",
    "Count the number of records where the city is Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('address.city') == \"Phoenix\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. (1 PT) Hours**  \n",
    "\n",
    "Count the number of records where closing time on Thursday is 8pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3313"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('hours.thursday_close') == 2000).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. (1 PT) Location and Hours**  \n",
    "\n",
    "Count the number of records where city is Phoenix and closing time on Thursday is 8pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((col('address.city') == \"Phoenix\") & (col('hours.thursday_close') == 2000)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. (1 PT) Price Range**  \n",
    "\n",
    "Price range is quoted in number of dollar signs.  Count the number of records with price range greater than or equal to two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: most of the price ranges are null, which is annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1135"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('menu.price_range') >= 2).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. (1 PT) COMPANY HEADQUARTERS**  \n",
    "\n",
    "For the `address.is_headquarters` field:  \n",
    "how many locations are HQ / are NOT HQ / are null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|is_headquarters|count|\n",
      "+---------------+-----+\n",
      "|           null|87625|\n",
      "|           true|  318|\n",
      "|          false|66736|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(col('address.is_headquarters')).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. (1 PT) Webpage URLs**  \n",
    "\n",
    "Register the dataframe as a temp table.  \n",
    "Next, use Spark SQL to select only the webpage title column, filtering on rows where the webpage url (accessed under `webpage.url`) is *Target.com*. \n",
    "\n",
    "Show only one resulting row and don't truncate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|title                          |\n",
      "+-------------------------------+\n",
      "|Target : Expect More. Pay Less.|\n",
      "+-------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"commerical\")\n",
    "\n",
    "spark.sql(\"SELECT webpage.title FROM commerical where webpage.url == 'Target.com'\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. (1 PT) Analysis on Ratings**  \n",
    "\n",
    "The reviews contains information such as the number of stars for each review (the *rating*).  \n",
    "The ratings are stored in an array (`reviews.stars`) for each business location (you should check for yourself). Return the top five most common rating arrays.  For example, an array might look like: \n",
    "[5, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we include `null` values*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| stars|count|\n",
      "+------+-----+\n",
      "|  null|74679|\n",
      "|    []|42419|\n",
      "|   [5]| 4258|\n",
      "|[null]| 3067|\n",
      "|[5, 5]| 1610|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(col('reviews.stars')).count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we **ignore** `null` values*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|    stars|count|\n",
      "+---------+-----+\n",
      "|      [5]| 4258|\n",
      "|   [5, 5]| 1610|\n",
      "|      [1]| 1559|\n",
      "|[5, 5, 5]|  836|\n",
      "|      [4]|  776|\n",
      "+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import exists\n",
    "\n",
    "df.filter(exists(col('reviews.stars'), lambda x: x.isNotNull())).groupBy(col('reviews.stars')).count().sort(col('count').desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. More work with Ratings**  \n",
    "\n",
    "For this question, you will filter out null ratings and then compute the average rating for each business location (using the field: `id`).\n",
    "\n",
    "\n",
    "a) (1 PT) Create a new dataframe retaining two fields: `id`, `reviews.stars`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_df = df.select(col('id'), col('reviews.stars'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (1 PT) Create a row for each rating  \n",
    "hint: use the `withColumn()` and `explode()` functions  \n",
    "you will need to import the `explode()` function by issuing:\n",
    "\n",
    "`from pyspark.sql.functions import explode`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|              id|stars|\n",
      "+----------------+-----+\n",
      "|000136e65d50c3b7|    4|\n",
      "|000136e65d50c3b7|    4|\n",
      "|0003b7589a4e12a0|    5|\n",
      "|00045f958e4bb02a| null|\n",
      "|00045f958e4bb02a| null|\n",
      "|00045f958e4bb02a| null|\n",
      "|00045f958e4bb02a| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4| null|\n",
      "|00059519f0dba1b4|    1|\n",
      "|00059519f0dba1b4|    5|\n",
      "|00059519f0dba1b4|    2|\n",
      "|00059519f0dba1b4|    4|\n",
      "|00059519f0dba1b4|    5|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "stars_df_exploded = stars_df.withColumn('stars', explode(col('stars')))\n",
    "stars_df_exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (1 PT) Return a count of the number of ratings in this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600082"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars_df_exploded.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) (1 PT) Drop rows where the rating is null, and return a count of the number of non-null ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "538241"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars_df_exploded_clean = stars_df_exploded.filter(col('stars').isNotNull())\n",
    "stars_df_exploded_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) (1 PT) Compute the average rating, grouped by `id`. After the average is computed, sort by `id` in ascending order and show the top 10 records.  \n",
    " \n",
    "hint:   \n",
    "this can all be done in one line using the `agg()` function  \n",
    "this `id` should be at the top: 000136e65d50c3b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|              id|        avg(stars)|\n",
      "+----------------+------------------+\n",
      "|000136e65d50c3b7|               4.0|\n",
      "|0003b7589a4e12a0|               5.0|\n",
      "|00059519f0dba1b4|3.3333333333333335|\n",
      "|000a1df4c8e0ecd2|               4.6|\n",
      "|000c7b7a30623083|               5.0|\n",
      "|000c9ffc8b89af03|               3.0|\n",
      "|000de20baa847ecc|1.6666666666666667|\n",
      "|001064359d9f162f|               5.0|\n",
      "|0010c9f495d87dd7|               3.0|\n",
      "|0017774db5e6400a| 4.333333333333333|\n",
      "+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# sorting by ascending id is the default behavior, so we don't need to specify that part\n",
    "# not sure why we'd need the agg() function since we can just the avg directly to average one column (stars col)\n",
    "stars_df_exploded_clean.groupBy(col('id')).avg().show(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
