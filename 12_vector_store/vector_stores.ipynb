{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: November 18, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SOURCES: \n",
    "\n",
    "- [Vector Stores](https://js.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "\n",
    "- [What is a Vector Database?](https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "- [What are vector embeddings](https://www.pinecone.io/learn/vector-embeddings-for-developers/)\n",
    "\n",
    "- [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "\n",
    "- [RAG pattern](https://vitalflux.com/retrieval-augmented-generation-rag-llm-examples/)\n",
    "\n",
    "### OBJECTIVES\n",
    "\n",
    "- Explain uses cases for vector embeddings\n",
    "- Explain the value of a vector store\n",
    "- Provide one example of how ANN works\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Vector embedding\n",
    "- Similarity of embeddings\n",
    "- Vector database\n",
    "- Approximate Nearest Neighbor (ANN)\n",
    "- Projection Matrix\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on Vector Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Natural Language Processing (NLP) classifiers used presence or count of words in documents as predictors.\n",
    "\n",
    "Keywords lose their context, which is often important\n",
    "\n",
    "A large leap forward used vector representations (**embeddings**) of documents\n",
    "\n",
    "The embeddings are vectors of fixed size like 64 or 128; values are floats.\n",
    "\n",
    "All kinds of media are now embedded: documents, videos, images, etc.\n",
    "\n",
    "Word embedding examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./embed_examples.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the elements are interpretable. \n",
    "\n",
    "In this example, (dog/puppy/cat) elements have similar sign & direction for some columns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects as Vectors\n",
    "\n",
    "In the figure below, each object is projected into 2D space as a vector.  \n",
    "There is a notion of object similarity which can be measured by distance between points.  \n",
    "The light blue objects (represented as points) are more similar than the other objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vector_space.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "\n",
    "Different embeddings can be compared using a similarity score like [*cosine similarity*](https://en.wikipedia.org/wiki/Cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cosine_sim.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training and Use\n",
    "\n",
    "Embeddings are formed by training a neural network on the data and taking the last hidden layer.\n",
    "\n",
    "This layer is a vector which encodes rich information\n",
    "\n",
    "One of the earliest models was [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "\n",
    "After objects are represented as vectors, they can be stored and reused later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow looks like this:\n",
    "\n",
    "```\n",
    "raw data -> embedding model -> vector embedding\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage\n",
    "\n",
    "The vectors can be stored in a traditional database (relational or NoSQL)...  \n",
    "...but specialized databases have emerged to efficiently store, compare, and search on embeddings.\n",
    "\n",
    "These are called **vector stores** or **vector databases**  \n",
    "\n",
    "Examples:\n",
    "\n",
    "- [Pinecone](https://www.pinecone.io/)\n",
    "- [OpenSearch](https://aws.amazon.com/opensearch-service/)\n",
    "\n",
    "We will look at a Pinecone demo in this module\n",
    "\n",
    "**Benefits of a vector database**\n",
    "\n",
    "- Optimized storage and querying\n",
    "\n",
    "- Vector databases can store metadata associated with each vector entry. Users can then query the database using additional metadata filters for finer-grained queries.\n",
    "\n",
    "-  real-time data updates,\n",
    "\n",
    "- designed to scale with growing data volumes and user demands, providing better support for distributed and parallel processing\n",
    "\n",
    "- Vector databases can more easily integrate with other components of a data processing ecosystem, such as ETL pipelines (like Spark), analytics tools\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Databases use Approximate Search\n",
    "\n",
    "In vector databases, we apply a similarity metric to find a vector that is the most similar to our query.\n",
    "\n",
    "It can be too costly to compare the query vector to each content vector, so approximations are often made.\n",
    "\n",
    "A vector database uses a combination of different algorithms to implement *Approximate Nearest Neighbor (ANN)* search.  \n",
    "Algorithms use techniques like projection, hashing, quantization, or graph-based search.  \n",
    "See [here](https://www.pinecone.io/learn/vector-database/) for details.\n",
    "\n",
    "Vector database indexes vectors using an algorithm such as *Random Projection*.\n",
    "\n",
    "**Random Projection**\n",
    "\n",
    "Based on this important lemma:  \n",
    "[*Johnsonâ€“Lindenstrauss lemma*](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma):  \n",
    "Set of points in high-dimensional space can be embedded into space of much lower dimension  \n",
    "while nearly preserving distances between points.\n",
    "\n",
    "This can be done by multiplying high-dim dataset by a lower-dimensional random matrix.\n",
    "\n",
    "From [Bingham and Mannila](https://cs-people.bu.edu/evimaria/cs565/kdd-rp.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='random_proj.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the dimensionality of the data is reduced, the search process is significantly faster than searching the entire high-dimensional space.\n",
    "\n",
    "It is also much faster than using PCA\n",
    "\n",
    "Projection quality depends on properties of the projection matrix\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Databases Need to Provide High Performance and Fault Tolerance\n",
    "\n",
    "As the amount of data increases, more nodes are required\n",
    "\n",
    "More nodes means increased risk of failure\n",
    "\n",
    "To ensure high performance and fault tolerance, vector databases use sharding and replication.\n",
    "\n",
    "*Sharding* - partitioning data across multiple nodes. Might save similar vectors in same partition.\n",
    "\n",
    "*Replication* - creating multiple copies of data across different nodes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embedding Use Cases\n",
    "\n",
    "Some important use cases are:\n",
    "\n",
    "- **Search** - Embeddings can represent deeper attributes of an object than keywords. \n",
    "  They can be much more effective in getting good search results like this:  \n",
    "  - User query can be embedded (using a specific embedding model)\n",
    "  - Each piece of content was embedded earlier (using that same embedding model)\n",
    "  - Similarity between query embedding and each content embedding is calculated\n",
    "  - Highest-scoring matches are selected\n",
    "  - Apply any relevant filters\n",
    "  - Return top results\n",
    "  \n",
    "- **Question answering** - better search allows for better ability to answer questions  \n",
    "  \n",
    "  \n",
    "- **Recommendation** - better search allows for more relevant recommendations  \n",
    "   Example: Given attribute information for users and items, recommend items with similar vectors\n",
    "  \n",
    "- **Generative AI** - GenAI models can produce new content and they can power chatbots  \n",
    "  One major risk is *hallucination*. If there is a request where the model wasn't sufficiently trained, it may return nonsense.  \n",
    "  Popular approach now is *RAG* - retrieval augmented generation   \n",
    "  \n",
    "  \n",
    "RAG does this:\n",
    "  - Embed the query\n",
    "  - Search for most similar content embedding\n",
    "  - Include matching content in user prompt: \"Based on the content below, tell me about neural networks.\"  \n",
    "  - Large language model (LLM) constructs and returns result based on prompt + context\n",
    "  \n",
    "**RAG Architecture Diagram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rag.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The use cases for vector embeddings continue to expand in exciting ways\n",
    "\n",
    "Generative AI has massively increased use and interest in vector databases\n",
    "\n",
    "Some common patterns like RAG have emerged\n",
    "\n",
    "These patterns bring together different components like storage and deep learning\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
