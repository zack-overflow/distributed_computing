{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Tuning and Topic Modeling\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**  \n",
    "In this assignment, you will do three things:\n",
    "1) Tune a logistic regression model  \n",
    "2) Label-balance a dataset  \n",
    "3) Run the Topic Modeling notebook, making small tweaks and capturing results  \n",
    "\n",
    "**TOTAL POINTS: 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 02:00:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/19 02:00:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"data preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update to match your path\n",
    "directory_path = '/sfs/qumulo/qhome/apt4c/ds5110/07_tuning_and_nlp_task/'\n",
    "full_path_to_file = os.path.join(directory_path, 'breast_cancer_wisconsin.csv')\n",
    "path_to_data = os.path.join(full_path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class = 2 for benign (negative class, 4 for malignant (positive class)\n",
    "target = 'class'\n",
    "positive_label = 4\n",
    "negative_label = 2\n",
    "\n",
    "SEED = 314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "brca = spark.read.csv(path_to_data, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- clump_thickness: integer (nullable = true)\n",
      " |-- uniformity_cell_size: integer (nullable = true)\n",
      " |-- uniformity_cell_shape: integer (nullable = true)\n",
      " |-- marginal_adhesion: integer (nullable = true)\n",
      " |-- single_epithelial_cell_size: integer (nullable = true)\n",
      " |-- bare_nuclei: string (nullable = true)\n",
      " |-- bland_chromatin: integer (nullable = true)\n",
      " |-- normal_nucleoli: integer (nullable = true)\n",
      " |-- mitoses: integer (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brca.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brca.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  241|\n",
      "|    2|  458|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute distribution of target variable\n",
    "brca.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Cross Validate a Logistic Regression Model\n",
    "i) (**4 PTS**) This task has the following requirements:\n",
    "- import necessary modules\n",
    "- use these features as predictors: `clump_thickness`,`uniformity_cell_size`,`uniformity_cell_shape`,`marginal_adhesion`  \n",
    "- `class` is response variable. apply recoding as needed. hint: save as new variable.\n",
    "- use 3 folds in the cross validator object\n",
    "- use BinaryClassificationEvaluator\n",
    "- logistic regression model with `maxIter`=10  \n",
    "- tuning grid with `regParam` values of 0.1 and 0.01\n",
    "- finally, print the average metrics based on each `regParam` value. the attribute `avgMetrics` in the cv model will hold these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:  Balancing a DataFrame with Downsampling  \n",
    "i) (**2 PTS**) Write a function to implement downsampling.  Enter code into the cell containing the `downsample` function.  \n",
    "\n",
    "INPUTS  \n",
    "* df               - Spark dataframe  \n",
    "* target           - string, target variable  \n",
    "* positive_label   - integer, value of positive label  \n",
    "* negative_label   - integer, value of negative label  \n",
    "\n",
    "OUTPUT  \n",
    "balanced spark dataframe  \n",
    "\n",
    "Downsampling = sample from larger class to match smaller class  \n",
    "\n",
    "**Example:**  \n",
    "\n",
    "INITIAL STATE  \n",
    "Smaller class has 100 records  \n",
    "Larger class size has 400 records\n",
    "\n",
    "ACTION  \n",
    "Sample 100 records from larger class, without replacement  \n",
    "Retain all records from smaller class\n",
    "\n",
    "END STATE    \n",
    "This produces a balanced dataset containing 100 records from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) **(1 PT)** Print the target distribution from this balanced dataset, to show the label counts nearly match.\n",
    "\n",
    "#### IMPORTANT NOTE:\n",
    "Sampling won't produce the exact fraction you request. In order to sample efficiently, Spark uses Bernouilli Sampling. \n",
    "Each row is assigned a probability of being included. If you request a 10% sample, each row individually has a 10% chance of being included but this does not guarantee an exact 10% sample   \n",
    "(it should be close, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Topic Modeling\n",
    "\n",
    "In this exercise, you will run the `topic_modeling.ipynb` notebook and answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) **(1 PT)** For the first headline in the dataset, the code processes it and extracts tokens. Provide a list of the tokens.\n",
    "\n",
    "+-------------+--------------------------------------------------+  \n",
    "|publish_date | headline_text                                     |  \n",
    "+-------------+--------------------------------------------------+  \n",
    "|20030219     | aba decides against community broadcasting licence|  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) **(1 PT)** The code created a count vectorizer and extracted features. \n",
    "\n",
    "`cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)`\n",
    "\n",
    "The first document had six tokens and the feature vector looked like this:\n",
    "\n",
    "(500, [118, 498], [1.0, 1.0])   \n",
    "\n",
    "Explain why there are only two non-zero elements in this feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) **(1 PT)** Change the number of topics to 2, rerun LDA, and visualize the topics by showing the topic words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
