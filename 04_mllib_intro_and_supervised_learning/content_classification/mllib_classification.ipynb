{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Intro and Classifiers\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: September 17, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES \n",
    "Learning Spark: Machine Learning with MLlib\n",
    "\n",
    "Logistic Regression using the DataFrame API  \n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduce classification examples using MLlib, including logistic regression\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Supervised learning\n",
    "- Binary and multiclass classification\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Tree Methods\n",
    "---\n",
    "\n",
    "**Game Plan**\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Briefly summarize some of the major classification models\n",
    "2. Illustrate how to work with some of these models using MLlib\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Machine Learning in Spark \n",
    "Spark MLlib is the library for machine learning.  There are two interfaces:\n",
    "\n",
    "1) A newer DataFrame-based API which is being actively built out\n",
    "\n",
    "2) An older RDD-based API which is still maintained, but it is not growing  \n",
    "  For supervised learning tasks*, the RDD API uses a `LabeledPoint` object to bundle labels with predictors.\n",
    "  \n",
    "  For unsupervised learning tasks, since there is no label, the `LabeledPoint` object is not used.  \n",
    "  Examples of unsupervised learning tasks include clustering methods like k-means.\n",
    "\n",
    "Some functionality is only available in the RDD-based API.  \n",
    "We will discuss both APIs in this course. \n",
    "\n",
    "\n",
    "(\\*) In *supervised learning* tasks, each observation has a label or ground truth indicating the correct answer.  \n",
    "Unsupervised learning tasks do NOT have this label. Most data in the wild does not have the label.\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Introduction to Classification\n",
    "\n",
    "Classification is a common form of supervised learning.  \n",
    "In supervised learning, the training examples include labels.  \n",
    "After training the model, the purpose of the task is to predict labels for new examples.  \n",
    "\n",
    "The data type of the $Y$ variable makes it a *classification problem*, namely $Y$ is a discrete variable.  \n",
    "Binary classification is most common. Examples include fraud (or not), default, survival, claim filing, spam.\n",
    "\n",
    "A continuous $Y$ variable results in a regression problem (next topic).\n",
    "\n",
    "In the RDD API, classification and regression both use the `LabeledPoint` class.  \n",
    "To remind ourselves, a `LabeledPoint` consists of a label and a feature vector.  \n",
    "\n",
    "Follow this convention for labels:  \n",
    "- For binary classification, use labels $0$ and $1$  \n",
    "- For multiclass classification, use labels $0$, $1$, …, $C-1$ where $C$ is the number of classes  \n",
    "\n",
    "\n",
    "\n",
    "Spark supports several popular models for classification including:  \n",
    "- Logistic regression  \n",
    "- Naive Bayes  \n",
    "- Tree methods (e.g., decision tree, random forest)  \n",
    "- Support Vector Machines  \n",
    "\n",
    "\n",
    "\n",
    "### 3) Logistic regression \n",
    "\n",
    "This is currently the most popular method for binary classification.  \n",
    "It is a generalized linear model which uses a linear plane to separate positive and negative examples.  \n",
    "Although the model is relatively simple, the results can be very competitive.  \n",
    "\n",
    "Below is an example of some data and a logistic curve fit to the data. Probability of Passing $Y$ is a function of Hours Studying $X$.  Notice the $Y$ variable consists of the values 0, 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg_img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Problems  \n",
    "The algorithm will output a multinomial logistic regression model, which contains $K−1$ binary logistic regression models regressed against the first class. Given a new data point, $K−1$ models will be run, and the class with largest probability will be chosen as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/20 14:22:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/20 14:22:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# MODULES, CONTEXT, AND PATHING\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression with RDD API**  \n",
    "\n",
    "We will load data/train model/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "data = sc.textFile('sample_svm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0',\n",
       " '0 2.857738033247042 0 0 2.619965104088255 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]\n"
     ]
    }
   ],
   "source": [
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Print a record to understand the data structure\n",
    "print(parsedData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/20 14:22:58 WARN Instrumentation: [6792f81e] Initial coefficients will be ignored! Its dimensions (1, 16) did not match the expected size (1, 16)\n"
     ]
    }
   ],
   "source": [
    "# Build the model using the stochastic gradient descent optimizer\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 1), (0.0, 1), (0.0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data. For each record, create tuple of (label, prediction)\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "print(labelsAndPreds.take(3))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with DataFrame API\n",
    "\n",
    "This will be the more common approach\n",
    "\n",
    "**The concepts and ideas are used for other ML models as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame API, the `LabeledPoint` object is NOT used.  \n",
    "Instead, the requirement is to package all predictor columns into a single column.  \n",
    "The ML model will take the predictor column name as an input, and the target variable name as an input.\n",
    "\n",
    "There are two transformations we should discuss right away, as they are very helpful:\n",
    "\n",
    "`VectorAssembler`  \n",
    "This will package the DataFrame predictor columns into a single column.\n",
    "\n",
    "\n",
    "`StandardScaler`  \n",
    "This will scale your data, and it can be applied after the VectorAssembler step.\n",
    "\n",
    "**EXAMPLE**  \n",
    "This small dataset has target variable *high_price*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/20 14:23:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|high_price|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|         1|       8.5552|              40.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|         1|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data into DF\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"ml_classifier\").getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "filename = \"sample_housing_data.csv\"\n",
    "\n",
    "# read data into dataframe\n",
    "training = spark.read.csv(filename,  inferSchema=True, header = True)\n",
    "training.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `VectorAssembler` to package some variables into a feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "|high_price|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|features      |\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "|1         |8.5552       |40.0              |880.0      |129.0         |322.0     |126.0     |37.88   |-122.23  |[8.5552,880.0]|\n",
      "|1         |8.3252       |41.0              |880.0      |129.0         |322.0     |126.0     |37.88   |-122.23  |[8.3252,880.0]|\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"median_income\", \"total_rooms\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(training)\n",
    "tr.select(\"*\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `StandardScaler` to scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------------------------+\n",
      "|high_price|features      |scaledFeatures                        |\n",
      "+----------+--------------+--------------------------------------+\n",
      "|1         |[8.5552,880.0]|[4.544281109921356,0.3640968833079785]|\n",
      "|1         |[8.3252,880.0]|[4.422111592518852,0.3640968833079785]|\n",
      "+----------+--------------+--------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(tr)\n",
    "scaledData = scalerModel.transform(tr)\n",
    "\n",
    "scaledData.select(\"high_price\",\"features\",\"scaledFeatures\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and fit the model. We discuss the parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.07027733010720466,0.0]\n",
      "Intercept: -0.954705346212893\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='high_price',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+\n",
      "|probability                             |prediction|\n",
      "+----------------------------------------+----------+\n",
      "|[0.6537005259491084,0.34629947405089156]|0.0       |\n",
      "|[0.6556415610201259,0.34435843897987406]|0.0       |\n",
      "|[0.6558421210389079,0.34415787896109207]|0.0       |\n",
      "|[0.664584376177192,0.335415623822808]   |0.0       |\n",
      "|[0.6778813168214649,0.3221186831785351] |0.0       |\n",
      "+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under PR Curve: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledData)\n",
    "lrPred.select('probability','prediction').show(5,truncate=False)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"high_price\",\n",
    "                                          metricName=\"areaUnderPR\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "aupr = evaluator.evaluate(lrPred)\n",
    "\n",
    "print(\"Area under PR Curve:\", aupr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Naive Bayes\n",
    "\n",
    "Naive Bayes (NB) is a relatively simple model, yet the performance can be quite good.  This has led to its popularity.  \n",
    "\n",
    "NB does multiclass classification. It is commonly used in text classification where the input features are count variables.\n",
    "\n",
    "At a high level, the count of a word on a page can adjust the probability that the page belongs to a given class.  For example, the presence of the word “tacos” will increase the probability that the page belongs to a **restaurant** relative to a **florist**.\n",
    "\n",
    "The algorithm computes the conditional probability distribution of each feature given a label, and then it applies Bayes’ theorem to compute the conditional probability distribution of a label given an observation.\n",
    "\n",
    "Naive?  \n",
    "The term “naive” comes from the simplifying assumption of independence between every pair of features. This assumption greatly simplifies the model and is often reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Implementation**  \n",
    "\n",
    "Several methods are supported including:\n",
    "\n",
    "- multinomial naive Bayes\n",
    "- Bernoulli naive Bayes\n",
    "\n",
    "**Parameters**  \n",
    "The model type is selected with an optional parameter “multinomial”, “complement”, “bernoulli” or “gaussian”, with “multinomial” as the default. \n",
    "\n",
    "For document classification, the input feature vectors should usually be sparse vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Example**\n",
    "\n",
    "We will load data/train model/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/20 14:23:08 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|label|            features|       rawPrediction|probability|prediction|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|  0.0|(692,[121,122,123...|[-225289.31301264...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[122,123,148...|[-179927.31719147...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-202007.18900696...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-277661.89434484...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-247993.96658158...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[-205425.55655384...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-210393.27785331...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[150,151,152...|[-155823.93925783...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[-215312.85347360...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[155,156,180...|[-230179.67219686...|  [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[119,120,121...|[-181043.98513836...|  [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[123,124,125...|[-139918.96412124...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-125730.20247452...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-84746.569691846...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-107047.60287297...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-80416.957124214...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-83826.752373160...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-145192.35184259...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-145429.26601505...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-81004.098436374...|  [0.0,1.0]|       1.0|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\") \\\n",
    "    .load(\"./sample_libsvm_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# set up the model with some parameter values\n",
    "nb = NaiveBayes(labelCol='label',\n",
    "                featuresCol='features',\n",
    "                smoothing=1.0, \n",
    "                modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# make predictions\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Tree Methods\n",
    "\n",
    "Tree methods can be used for both classification and regression  \n",
    "\n",
    "Simplest method is a Decision Tree, which is intuitively appealing due to series of binary decisions (Male/Female, Age greater than 30 or not)  \n",
    "\n",
    "Can handle missing values (in many implementations), categorical data, continuous data.  \n",
    "Minimal preprocessing needed.  \n",
    "\n",
    "Feature selection is part of algorithm (best feature is used, then next best, …)  \n",
    "\n",
    "Does not require scaling  \n",
    "Handles non-linear interactions  \n",
    "Handles multiclass classification  \n",
    "\n",
    "Code examples can be found in the documentation. Random forest code, for example, can be found here:\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TRY FOR YOURSELF (UNGRADED EXERCISE)**\n",
    "\n",
    "Copy all DataFrame API logistic regression code in the cell below, modify the input columns in VectorAssembler, and refit the model.  \n",
    "Compute and print the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
