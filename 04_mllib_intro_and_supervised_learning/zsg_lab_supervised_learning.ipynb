{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Lab: Supervised Learning\n",
    "### Last Updated: August 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "#### Submission\n",
    "\n",
    "Zack Gottesman\n",
    "qdw5jf\n",
    "\n",
    "credit to Prof. Tashman for code inpsired by class notebooks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/26 22:30:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "We'll go as long as possible before splitting train and test data to make sure we are making the same modifications to each. Eventually we will have to split because we don't want to leak information about the test data during training. We'll build a pipeline to handle things after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(DATA_FILEPATH, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+\n",
      "|   f1|   f2|diagnosis|\n",
      "+-----+-----+---------+\n",
      "|17.99|10.38|        M|\n",
      "|20.57|17.77|        M|\n",
      "|19.69|21.25|        M|\n",
      "+-----+-----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kept_data = df.select([\"f1\", \"f2\", \"diagnosis\"])\n",
    "kept_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kept_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|diagnosis|\n",
      "+---------+\n",
      "|        B|\n",
      "|        M|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kept_data.select(\"diagnosis\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean up data and convert to correct types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to change columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+------+------+\n",
      "|   f1|   f2|diagnosis|f1_int|f2_int|\n",
      "+-----+-----+---------+------+------+\n",
      "|17.99|10.38|        M| 17.99| 10.38|\n",
      "|20.57|17.77|        M| 20.57| 17.77|\n",
      "|19.69|21.25|        M| 19.69| 21.25|\n",
      "+-----+-----+---------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as typ\n",
    "\n",
    "converted_data = kept_data.withColumn(\"f1_int\", kept_data[\"f1\"].cast(typ.FloatType()))\n",
    "converted_data = converted_data.withColumn(\"f2_int\", kept_data[\"f2\"].cast(typ.FloatType()))\n",
    "converted_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to OHE the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+------+------+-------------+\n",
      "|   f1|   f2|diagnosis|f1_int|f2_int|diagnosis_idx|\n",
      "+-----+-----+---------+------+------+-------------+\n",
      "|17.99|10.38|        M| 17.99| 10.38|          1.0|\n",
      "|20.57|17.77|        M| 20.57| 17.77|          1.0|\n",
      "|19.69|21.25|        M| 19.69| 21.25|          1.0|\n",
      "+-----+-----+---------+------+------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "idxer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"diagnosis_idx\")\n",
    "converted_data = idxer.fit(converted_data).transform(converted_data)\n",
    "converted_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "|f1_int|f2_int|diagnosis_idx|\n",
      "+------+------+-------------+\n",
      "| 17.99| 10.38|          1.0|\n",
      "| 20.57| 17.77|          1.0|\n",
      "| 19.69| 21.25|          1.0|\n",
      "| 11.42| 20.38|          1.0|\n",
      "| 20.29| 14.34|          1.0|\n",
      "+------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data = converted_data.select(\"f1_int\", \"f2_int\", \"diagnosis_idx\")\n",
    "clean_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = clean_data.randomSplit([0.6, 0.4], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "|f1_int|f2_int|diagnosis_idx|\n",
      "+------+------+-------------+\n",
      "| 6.981| 13.43|          0.0|\n",
      "| 7.691| 25.44|          0.0|\n",
      "| 7.729| 25.49|          0.0|\n",
      "+------+------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+\n",
      "|f1_int|f2_int|diagnosis_idx|\n",
      "+------+------+-------------+\n",
      "|  7.76| 24.54|          0.0|\n",
      "| 8.219|  20.7|          0.0|\n",
      "| 8.598| 20.98|          0.0|\n",
      "+------+------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Data Transformations\n",
    "We use `VectorAssembler` to prepare the predictors and `StandardScaler` to scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "vec_ass = VectorAssembler(inputCols=[\"f1_int\", \"f2_int\"], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol='diagnosis_idx',\n",
    "                        featuresCol='scaled_features',\n",
    "                        fitIntercept=True,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict on test data\n",
    "\n",
    "We have to modify the test data in the same way as training data to make predictions using our model. Pipelines are made for that!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe = Pipeline(stages=[vec_ass, scaler, lr])\n",
    "pipe_model = pipe.fit(train_df)\n",
    "preds = pipe_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------------------------------+----------+-------------+\n",
      "|rawPrediction                           |probability                               |prediction|diagnosis_idx|\n",
      "+----------------------------------------+------------------------------------------+----------+-------------+\n",
      "|[5.884640921984568,-5.884640921984568]  |[0.9972258722478972,0.0027741277521028396]|0.0       |0.0          |\n",
      "|[3.054710803703724,-3.054710803703724]  |[0.9549854701951765,0.045014529804823455] |0.0       |0.0          |\n",
      "|[3.117204964756265,-3.117204964756265]  |[0.9575968805319068,0.042403119468093164] |0.0       |0.0          |\n",
      "|[2.682368750679835,-2.682368750679835]  |[0.9359782129093271,0.06402178709067285]  |0.0       |0.0          |\n",
      "|[2.945410642126358,-2.945410642126358]  |[0.9500461338150327,0.04995386618496733]  |0.0       |0.0          |\n",
      "|[3.1261572168294727,-3.1261572168294727]|[0.957958902129706,0.042041097870293975]  |0.0       |0.0          |\n",
      "|[2.546475979775778,-2.546475979775778]  |[0.927336410807531,0.07266358919246896]   |0.0       |0.0          |\n",
      "|[1.36740145702991,-1.36740145702991]    |[0.796959995415952,0.20304000458404803]   |0.0       |0.0          |\n",
      "|[1.1497843451450613,-1.1497843451450613]|[0.7594715245046884,0.24052847549531164]  |0.0       |0.0          |\n",
      "|[-1.0001245080919787,1.0001245080919787]|[0.2689169422975888,0.7310830577024112]   |1.0       |1.0          |\n",
      "|[-1.1451682836229615,1.1451682836229615]|[0.24137272204076538,0.7586272779592347]  |1.0       |1.0          |\n",
      "|[-7.033061413859322,7.033061413859322]  |[8.814491428129087E-4,0.999118550857187]  |1.0       |1.0          |\n",
      "|[-2.552539392324924,2.552539392324924]  |[0.07225607184876945,0.9277439281512305]  |1.0       |1.0          |\n",
      "|[-3.078022876448049,3.078022876448049]  |[0.0440229475251095,0.9559770524748905]   |1.0       |1.0          |\n",
      "|[-3.8983709439334397,3.8983709439334397]|[0.01987201023903748,0.9801279897609625]  |1.0       |1.0          |\n",
      "|[-5.1589830514664,5.1589830514664]      |[0.005714696214109786,0.9942853037858902] |1.0       |1.0          |\n",
      "|[-9.569375091227116,9.569375091227116]  |[6.983013388568256E-5,0.9999301698661143] |1.0       |1.0          |\n",
      "+----------------------------------------+------------------------------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at some example predictions\n",
    "preds.select(\"rawPrediction\", \"probability\", \"prediction\", \"diagnosis_idx\") \\\n",
    "     .sample(fraction=0.05) \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.9540261654689511\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# area under ROC is default metric, but we specify anyway for clarity\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
    "                                          labelCol=\"diagnosis_idx\",\n",
    "                                          metricName=\"areaUnderROC\"\n",
    "                                         )\n",
    "\n",
    "auc = evaluator.evaluate(preds)\n",
    "\n",
    "print(\"Area under ROC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/26 23:37:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "# specify schema here bc inferred schema makes everything strings (ugh)\n",
    "house_schema = StructType([\n",
    "    StructField(\"median_house_value\", FloatType()),\n",
    "    StructField(\"median_income\", FloatType()),    \n",
    "    StructField(\"housing_median_age\", FloatType()),    \n",
    "    StructField(\"total_rooms\", FloatType()),    \n",
    "    StructField(\"total_bedrooms\", FloatType()),    \n",
    "    StructField(\"population\", FloatType()),\n",
    "    StructField(\"households\", FloatType()),        \n",
    "    StructField(\"latitude\", FloatType()),        \n",
    "    StructField(\"longitude\", FloatType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(DATA_FILEPATH2, header=True, schema=house_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "|          352100.0|       7.2574|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_house_value: float (nullable = true)\n",
      " |-- median_income: float (nullable = true)\n",
      " |-- housing_median_age: float (nullable = true)\n",
      " |-- total_rooms: float (nullable = true)\n",
      " |-- total_bedrooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+\n",
      "|median_house_val_scaled|median_house_value|\n",
      "+-----------------------+------------------+\n",
      "|                  4.526|          452600.0|\n",
      "|                  3.585|          358500.0|\n",
      "|                  3.521|          352100.0|\n",
      "+-----------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SCALE_FACTOR = 100000\n",
    "df2_scaled = df2.withColumn(\"median_house_val_scaled\", df2[\"median_house_value\"] / SCALE_FACTOR)\n",
    "df2_scaled.select(\"median_house_val_scaled\", \"median_house_value\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-----------------------+-----------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|median_house_val_scaled|     rooms_per_hh|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-----------------------+-----------------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|                  4.526|6.984126984126984|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|                  3.585|6.238137082601054|\n",
      "|          352100.0|       7.2574|              52.0|     1467.0|         190.0|     496.0|     177.0|   37.85|  -122.24|                  3.521|8.288135593220339|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-----------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_scaled_room_per_hh = df2_scaled.withColumn(\"rooms_per_hh\", df2_scaled[\"total_rooms\"] / df2_scaled[\"households\"])\n",
    "df2_scaled_room_per_hh.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df2, test_df2 = df2_scaled_room_per_hh.randomSplit([0.8, 0.2], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = [\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_hh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "vec_ass2 = VectorAssembler(inputCols=feats, outputCol=\"features\")\n",
    "scaler2 = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression(labelCol='median_house_val_scaled',\n",
    "                        featuresCol='scaled_features',\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0.8\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe2 = Pipeline(stages=[vec_ass2, scaler2, lin_reg])\n",
    "pipe_model2 = pipe2.fit(train_df2)\n",
    "preds2 = pipe_model2.transform(test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+\n",
      "|        prediction|median_house_val_scaled|\n",
      "+------------------+-----------------------+\n",
      "|1.2181898031384244|                  0.225|\n",
      "|  1.34823728251112|                  0.427|\n",
      "|1.5283115231160926|                  0.475|\n",
      "| 1.521477100832544|                  0.516|\n",
      "|1.4010033343161403|                  0.517|\n",
      "|1.4824352091423285|                  0.524|\n",
      "| 1.403050900171543|                  0.526|\n",
      "| 1.498483611805543|                  0.538|\n",
      "|1.4619872908242135|                  0.554|\n",
      "|1.5981774347782942|                  0.587|\n",
      "|1.5651121950566944|                  0.594|\n",
      "|1.5996992885291248|                  0.603|\n",
      "|1.6352271091456378|                  0.606|\n",
      "| 1.632626189904221|                   0.62|\n",
      "|1.4989816496438495|                  0.627|\n",
      "|1.4690430742530616|                  0.666|\n",
      "| 1.720837105496604|                   0.67|\n",
      "|1.9039826948846805|                  0.673|\n",
      "|1.2965780034477739|                  0.675|\n",
      "| 1.631740745323023|                  0.692|\n",
      "+------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at some example predictions\n",
    "preds2.select(\"prediction\", \"median_house_val_scaled\") \\\n",
    "     .sample(fraction=0.05) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.7551749818714476\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluatorMSE = RegressionEvaluator(labelCol=\"median_house_val_scaled\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluatorMSE.evaluate(preds2)\n",
    "print(\"MSE\", mse)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
