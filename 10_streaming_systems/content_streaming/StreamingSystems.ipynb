{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Systems\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: November 6, 2024\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "\n",
    "Streaming Systems by Tyler Akidau, Slava Chernyak, Reuven Lax\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Understand the definition of a streaming system, and how it is different from a batch system\n",
    "- Differentiate tables from streams\n",
    "- Understand why persistent state is essential in streaming systems\n",
    "- Understand the tradeoffs between different methods of persisting data\n",
    "- Differentiate between perfect watermarks and heuristic watermarks\n",
    "- Understand how different accumulation modes work\n",
    "- Present a short example of a windowed calculation using `Apache Beam`\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Batch processing\n",
    "- Streaming system\n",
    "- Streams and Tables\n",
    "- Persistent State\n",
    "- Event Time vs Processing Time\n",
    "- Windowing\n",
    "- Sessions\n",
    "- Triggers\n",
    "- Watermarks\n",
    "- Accumulation\n",
    "- Exactly-Once Processing\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing\n",
    "\n",
    "Before learning about streaming systems, let's briefly define *batch processing*.  \n",
    "\n",
    "Batch processing takes a finite dataset and processes it fully and all at once.  \n",
    "\n",
    "An alternative would be to separately process each record one at a time.\n",
    "\n",
    "The introduction of datasets designed to be infinite in cardinality pose challenges to this mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming System\n",
    "\n",
    "A *streaming system* is a data processing engine designed with infinite datasets in mind. Examples will include social data on the web, such as Twitter and Facebook.  These websites can be treated as accumulating continuous data streams.   \n",
    "\n",
    "Streaming systems include *microbatch* implementations such as *Spark Streaming* discussed later.  \n",
    "\n",
    "In microbatch, a batch processing engine is repeatedly called at a fairly high frequency, such as every 500 microseconds.  \n",
    "Each microbatch consists of a chunk of data, formed from a window processing time.  \n",
    "\n",
    "Once data is regarded as infinite, it adds complexity such as:  \n",
    "- what to persist (not all data can be stored)\n",
    "- when to aggregate and report results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streams and Tables\n",
    "\n",
    "**Tables** are data *at rest,* reflecting the data at a point in time.  Think of tables in a relational database.  \n",
    "\n",
    "**Streams** are data *in motion*.  They capture the evolution of data over time.  \n",
    "\n",
    "Aggregating a stream of updates over time produces a table.  \n",
    "\n",
    "Observing changes to a table over time produces a stream.\n",
    "\n",
    "**MapReduce Viewed as Streams and Tables**\n",
    "\n",
    "The stages look like this:  \n",
    "\n",
    "1. Consume a table of input data, such as sentences from a corpus\n",
    "2. Preprocess the input data into key/value form.  Preprocessing transforms the data into a stream.\n",
    "3. Consume the key/value pairs, outputting modified key/value pairs in form\n",
    "(word, 1). These are streams.\n",
    "4. Shuffle the data, sending key/value pairs w same key to same worker.\n",
    "5. Reduce by key, which is an aggregation\n",
    "6. Persist the data, producing a table\n",
    "\n",
    "The process starts with a table and ends with a table, using streams in between to transform the data.\n",
    "\n",
    "**Transformations**\n",
    "\n",
    "Transformations tell us what the pipeline is computing.  There are two kinds:\n",
    "\n",
    "- *Nongrouping* transformations take a stream as input and produce a new stream as output.  Examples include `filter`, `map`.  \n",
    "\n",
    "\n",
    "- *Grouping* transformation take a stream as input and perform aggregation, which transforms data into a table.  An example is `ReduceByKey`.\n",
    "\n",
    "**To repeat, grouping is what produces tables.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent State\n",
    "\n",
    "The purpose of persisting tables is to capture data that otherwise would vanish.\n",
    "There are a few reasons we need to do this:\n",
    "\n",
    "- Durability upon Interruption\n",
    "\n",
    "Streaming systems are supposed to run forever, but this isn't realistic in practice.  Interruptions happen for many reasons, such as machine failure, planned maintenance, code changes, and bad commands.\n",
    "\n",
    "Persistent state helps the system to recover from an interruption.\n",
    "\n",
    "For expensive, complex pipelines, can use *checkpointing* to periodically save results.  In event of failure, the system only loses data since the last checkpoint.  Persistence must be strategic, since it must be assumed that data through the system cannot be reloaded.\n",
    "\n",
    "- Correctness and Efficiency\n",
    "\n",
    "By persisting the necessary intermediate quantities, the system the can recover from an interruption.  It is important to save just what is necessary, since extra data just takes up space.\n",
    "\n",
    "**Raw Grouping versus Incremental Combining**\n",
    "\n",
    "*Raw grouping* appends each new element.  This can require massive storage costs.\n",
    "\n",
    "*Incremental combining* will incrementally compute and checkpoint an intermediate result.\n",
    "\n",
    "*Running Average*  \n",
    "As an example, if we are interested in the average of a vector of values, it will be more efficient to store a running sum and count (incremental combining) than to store all of the values (raw grouping).\n",
    "\n",
    "*Histogram*  \n",
    "A histogram will be a more complex accumulator than a running average, but it provides a better description of the distribution than the mean.  To persist a histogram, store a running count for each bucket range.\n",
    "\n",
    "*Parallelization*  \n",
    "Parallelization can be used to optimize aggregation.  Specifically, subgroups of data can be distributed across multiple machines, with each machine computing a partial aggregate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion Attribution\n",
    "\n",
    "Conversion attribution is used in the advertising technology (AdTech) space to provide concrete feedback on the efficacy of advertisements.  It is a common use case for streaming systems.  In the ideal situation, a desired outcome (consumer purchases a product) can be precisely traced back to an advertisement.  In practice, this task is often challenging for reasons such as a complex path between the advertisement (a.k.a. the *impression*) and the outcome, and *attribution fraud*.  \n",
    "\n",
    "The figure below depicts several user paths, with one conversion path leading from an impression to the outcome, or goal.  In this path, there are two additional steps between the impression and goal, illustrating that the task is tricky.  Other paths include: \n",
    "\n",
    "1) impression leading to a sequence of site visits, none ending with the goal state  \n",
    "2) user going directly to the goal, without ever getting the impression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/conv_attrib.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion Attribution with Apache Beam**\n",
    "\n",
    "The authors of the *Streaming Systems* text have developed `Apache Beam` for batch and streaming data processing jobs.\n",
    "The system is designed to run on any execution engine.  For details, see: https://beam.apache.org  \n",
    "\n",
    "One of the use cases explored in the text, for which source code is included, is conversion attribution.\n",
    "The source code is lengthy and written in Java.  Without the textbook, it might be a challenging read, but I will provide the link to the repo:\n",
    "https://github.com/takidau/streamingbook/blob/master/src/main/java/net/streamingbook/StateAndTimers.java\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "For a streaming system to be at parity with batch processing, it needs *correctness*.  In other words, batch systems process finite datasets, and it is straightforward to recompute results if needed.  With infinite datasets, care must to taken to recover needed data for correct results.  This is solved by  checkpointing persistent state over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Time versus Processing Time\n",
    "\n",
    "For any data processing system, there are two times of interest:\n",
    "\n",
    "1. *event time*, which is the time at which events actually occurred\n",
    "\n",
    "2. *processing time*, which is the time at which events are observed by the system\n",
    "\n",
    "Ideally these times would always coincide (black dashed line in **Processing Time vs Event Time figure below**), but in practice we observe a red curve.  For example, a user might use an app offline in airplane mode.  When the app switches back online, it uploads user statistics to the system for processing.  This causes the processing time to occur after the event time, known as *processing-time lag.*  *Event-time skew* measures the time between when the event actually occurred and when it was processed.\n",
    "\n",
    "The degree of event-time skew can be affected by several factors including:\n",
    "\n",
    "- shared resource limitations like network congestion, shared CPU in a nondedicated environment  \n",
    "- software causes such as distributed system logic\n",
    "- variance in throughput\n",
    "\n",
    "Not all use cases care about event times, but there are important cases that do, such as billing and anomaly detection.  \n",
    "\n",
    "**Processing Time vs Event Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/event_time_vs_proc_time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching Data and Windowing\n",
    "\n",
    "To deal with infinite datasets, processing systems use *windowing*, which chops the dataset into finite pieces along temporal boundaries.  Many systems (including Spark Streaming) define these temporal boundaries using processing time, as this is easier than defining by event time.\n",
    "\n",
    "Forming windows by processing time can lead to incorrect conclusions in the context of their event times.  From the lens of event times, they might arrive for processing out of order.  Further, there may not be certainty when all of the events from a given time window have all arrived for processing (there could be late data).  Streaming systems like `Apache Beam` aim at providing event time correctness.\n",
    "\n",
    "In the case of processing finite (bounded) data with a batch engine, unstructured data arrives and is run through the engine once, producing structured data.\n",
    "\n",
    "**Bounded Data Processing w Batch Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/bounded_data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing infinite (unbounded) data, a popular approach is to window the data into fixed-size windows and process each window as a bounded data source.  Each filename can contain the time window (process_x_1000_1010).\n",
    "\n",
    "A limitation with this approach is that delayed data will be included in the incorrect bucket, due to processing-time lag. To account for the lag, the system could attempt to:\n",
    "\n",
    "- delay processing until all events for that window have been collected  \n",
    "\n",
    "However, the time delay may not be practical, and it may never be known when all events from a window have been collected\n",
    "\n",
    "- reprocess the batch for a window when data arrives late.  \n",
    "\n",
    "This may be computationally expensive/prohibitive\n",
    "\n",
    "**Processing Windows w Batch Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/unbounded_fixed_windows.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sessions* are a more sophisticated windowing strategy.  A session begins when a user interacts with the system, and ends when the user hasn't interacted with the system for some predefined period of time, or *session timeout*.\n",
    "\n",
    "When a batch engine is used to process sessions, the sessions oftentimes get split into multiple windows, destroying structure.  This can be mitigated by increasing batch size (at the cost of increased latency), or repairing split sessions with logic (at the cost of increased complexity).\n",
    "\n",
    "\n",
    "**Processing Sessions w Batch Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/session.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, both the fixed window approach and session approach have shortcomings in terms of correctness.  This is because:\n",
    "\n",
    "- data may arrive for processing out of order with respect to their event times\n",
    "\n",
    "- the lateness of the data will vary, meaning the system cannot say that X% of the events will arrive within Y time units\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers\n",
    "\n",
    "A *trigger* is a mechanism for declaring when the output for a window should be materialized.  For example, when sufficient events have been collected for a window, a trigger will signal that a summary statistic should be computed and reported.\n",
    "\n",
    "There are generally two useful types of triggers:\n",
    "\n",
    "- *Repeated update triggers*  \n",
    "These periodically generate updated window panes as more data arrives.  \n",
    "These triggers are the most common type of trigger in streaming systems.  \n",
    "\n",
    "\n",
    "- *Completeness triggers*  \n",
    "These materialize a window pane after the input for that window is believed to be sufficient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarks\n",
    "\n",
    "When event times and processing times coincide, things are straightforward.  However, when event data can arrive late, the problem becomes tricky.  How do we know when to close a time window?  This motivates the concept of *watermarks*.\n",
    "\n",
    "Watermarks are temporal notions of input completeness in the event-time domain.  It makes the statement that all data with event times `<T` have been observed.  \n",
    "\n",
    "Sometimes watermarks are exact, but other times the best we can do is design approximate (heuristic) watermarks.\n",
    "\n",
    "Watermarks work together with triggers: when the watermark reaches a given level, the trigger fires, producing results. That is, output for one or more windows is materialized.\n",
    "\n",
    "Oftentimes, it makes sense to provide a bound on how late an event can arrive.  This allows the system to close the window after the fixed period, dropping any points exceeding the bound.\n",
    "\n",
    "**A Watermark Example**\n",
    "\n",
    "The watermark illustration below shows an example of a perfect watermark and a heuristic watermark.  In each case, the x-axis shows the event time domain sliced into fixed window panes of 2 minutes. The y-axis shows the processing time domain.  The circles denote data points, with their values inside.  The curves are the watermarks.\n",
    "\n",
    "Once a window is closed (with time denoted on the y-axis), results can be reported.  The numbers in yellow denote the values reported from each window.  For the perfect watermark, a value of 14 is reported for the 12:00-12:02 event time window a bit before 12:09.  This contrasts with the value of 5 reported for that same window using the heuristic watermark.  This is due to the red data point with value 9 arriving very late; the heuristic watermark missed that point, and it was ignored.  One advantage, however, is the earlier reporting time at 12:06. Thus, **the heuristic watermark faces a tradeoff between correctness and latency**.\n",
    "\n",
    "In real systems, perfect watermarks are often impractical.  The more that is known about the source, the better the heuristic that can be designed, resulting in fewer late data points.\n",
    "\n",
    "The most important distinction between perfect watermarks and heuristic watermarks is this: **perfect watermarks account for all data, while heuristic watermarks admit late data.**\n",
    "\n",
    "**Illustration of Watermarks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/watermarks.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Beam Code Example**\n",
    "\n",
    "The code below shows how a windowed computation is implemented in `Apache Beam`.\n",
    "A `PCollection` is a (possibly massive) dataset.  We parse key/value data where the key is named `Group` and the value is named `Score`; the values are integers.  The data are windowed into fixed, two-minute windows.  The summation is done by key.  After understanding the concepts (windows, reduce-by-key aggregation), the code is reasonably clear and appealing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Windowed summation code**\n",
    "```\n",
    "PCollection<KV<Group, Score>> totals = input\n",
    "  .apply(Window.into(FixedWindows.of(TWO_MINUTES)))\n",
    "  .apply(Sum.integersPerKey());\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulation\n",
    "\n",
    "When multiple panes are produced for a single window over time (say a statistic is refreshed with more data), we need an *accumulation* method.  Examples include:\n",
    "\n",
    "- *discarding*: each time a window pane is materialized, any stored (earlier) state is discarded.\n",
    "- *accumulating*: all stored states are retained, and the current state is the accumulation of all previous states.\n",
    "- *accumulating and retracting*: similar to accumulating mode, this returns the accumulation, and also retractions for the previous pane(s).\n",
    "\n",
    "**Accumulation Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the example below where two panes are produced for a window. The second pane is an update which occurs at a later processing time.  \n",
    "\n",
    "For `discarding` mode, pane 2 contains the updated value.  The correct final value is found by summing over the panes (= 3 + 9).  \n",
    "\n",
    "For `accumulating` mode, the final value has done the summing, and there is no need to sum over the panes. In fact, summing over the panes would produce 15, which is incorrect.  \n",
    "\n",
    "In `accumulating & retracting` mode, once again the final value has done the summing.  However, a sum over the panes computes the correct answer of 12, since it would sum [3, 12, -3].\n",
    "\n",
    "|    |      Discarding      |  Accumulating | Accumulating & Retracting |  \n",
    "|----------|:-------------:|:-------------:|:-------------:|  \n",
    "| Pane 1      |  3   | 3 | 3 |   \n",
    "| Pane 2      |    9 | 12 | 12, -3 |  \n",
    "| Final value | 9 | 12 | 12|  \n",
    "| Sum over panes | 12 | 15 | 12|  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactly-Once Processing\n",
    "\n",
    "Streaming systems reference *exactly-once processing*, meaning that every record is processed exactly once.  \n",
    "\n",
    "Historically, no guarantees were made about record processing; they were on a *best efforts* basis.  Records might be duplicated, producing incorrect aggregations.  If a machine crashed, aggregations might be lost, and records might need to be processed more than once.\n",
    "\n",
    "`Apache Beam` ensures that records are not erroneously dropped or duplicated.\n",
    "The user can configure how long the system should wait for late data.  Any records arriving later than this deadline are dropped.  This feature leads to potential inaccuracy, but assures that all records showing up on time for processing are accurately processed exactly once.  Late records are explicitly dropped."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
