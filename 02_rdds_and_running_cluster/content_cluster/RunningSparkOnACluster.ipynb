{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Spark on a Cluster\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES \n",
    "\n",
    "1. Learning Spark 1st ed., Chapter 7: Running on a Cluster\n",
    "2. [Cluster overview](https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "\n",
    "### OBJECTIVES\n",
    "- Learn how to run distributed Spark\n",
    "- Learn about some of the common deployment environments\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Cluster manager (Hadoop YARN, Apache Mesos, Standalone)\n",
    "- Driver and worker/executor\n",
    "- Spark application\n",
    "- Jobs, stages, and tasks\n",
    "- Directed acyclic graph (DAG)\n",
    "- Amazon Web Services tools for running Spark: EC2, EMR\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Architecture\n",
    "\n",
    "One benefit of Spark is the ability to scale computation by adding more machines and running in cluster mode\n",
    "\n",
    "The *workers (aka executors)* receive code and data chunks and do the processing, sending results back to driver.\n",
    "\n",
    "The *driver* is in charge of coordinating the workers\n",
    "\n",
    "Driver + Workers = Spark application\n",
    "\n",
    "### Driver\n",
    "\n",
    "`main()` method of program runs on driver\n",
    "\n",
    "Converts program into tasks\n",
    "\n",
    "Converts into logical *directed acyclic graph* (DAG) of operations\n",
    "\n",
    "Coordinates scheduling of tasks on executors (like a manager)\n",
    "\n",
    "### Executors\n",
    "\n",
    "Run the individual tasks\n",
    "\n",
    "Launch at start of application and run for lifetime of app\n",
    "\n",
    "Provide in-memory (RAM) storage for RDDs\n",
    "\n",
    "Using RAM speeds up computation versus slower disk\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "External service where the Spark application runs.  \n",
    "\n",
    "Spark is packaged with the Standalone cluster manager.\n",
    "\n",
    "Manages the resources between Spark applications.  \n",
    "Can manage queues if there is more demand than resources for executors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Cluster Overview. Source: Apache Spark.\n",
    "\n",
    "The driver program distributes data to workers.  \n",
    "On each worker is an executor.  \n",
    "It is possible to cache data in RAM for speedup (avoiding recompute).  \n",
    "The Cluster Manager is responsible for managing components of a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"cluster_manager.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Job\n",
    "\n",
    "For implementing the work, the *Job* is divided into *Stages*, which are further divided into *Tasks*.  \n",
    "Smallest unit of work is the Task.  \n",
    "Executors run the Tasks.\n",
    "\n",
    "**Example:**  \n",
    "Consider this line of code which reads a text file into an RDD and collects the data to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"code_read_textfile_into_rdd.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark comes with a built-in Web UI.  There are several tabs such as `Jobs` and `Stages` which provide details about the running application.  \n",
    "Useful information such as resources used at each stage of the computation is available here.\n",
    "\n",
    "When running jobs locally (*local mode*), you should be able to view the UI at this URL:  \n",
    "http://localhost:4040/jobs/\n",
    "\n",
    "\n",
    "From the UI, here are details on the Stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stages_read_textfile_into_rdd.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the UI, here are details on the Executors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"executor_info.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Launching a Program\n",
    "\n",
    "We generally run code from notebooks in this course.\n",
    "\n",
    "For running at command line, `spark-submit` is called to launch a Spark app\n",
    "\n",
    "**Run in local mode using single core**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit --master local python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in local mode using 4 cores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ bin\\spark-submit --master local[4] python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in local mode using all cores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit --master local[*] python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on Spark Standalone cluster at default port**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ bin\\spark-submit --master spark://host:7077 python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on Spark Standalone cluster at default port, specifying memory to allocate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit --master spark://host:7077 â€“-executor_memory 10g \tpython_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generic Form to run Spark App**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit [options] <app jar | python file> [app options]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging Code and Dependencies  \n",
    "\n",
    "**Python**  \n",
    "PySpark uses Python on worker machines, so can use `pip` for managing packages    \n",
    "Can also submit libraries using the `--py-files` argument to `spark-submit`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop YARN\n",
    "\n",
    "**Y**et **A**nother **R**esource **N**egotiator \n",
    "\n",
    "`YARN` is a cluster manager introduced in `Hadoop 2.0`  \n",
    "It does the following:\n",
    "- allocates system resources to various applications running in a `Hadoop` cluster.  \n",
    "- schedules tasks to be executed on different cluster nodes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon EC2 (elastic cloud compute)\n",
    "\n",
    "One of many services from Amazon Web Services (AWS) is EC2\n",
    "\n",
    "Spark has built-in script to launch clusters on EC2: `spark-ec2`\n",
    "\n",
    "Will need Amazon Web Services (AWS) account  \n",
    "Export the *access key ID* and *secret access key*    \n",
    "By default, launching the cluster produces one master and one worker  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS and the Free Tier\n",
    "\n",
    "AWS offers over 200 services in storage, compute, machine learning, and many other areas of tech including AI/ML.  \n",
    "\n",
    "There is an AWS Free Tier where some of the services are completely free.\n",
    "\n",
    "We will use this Free Tier for the course. Visit here to sign up:\n",
    "\n",
    "https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon Elastic MapReduce (EMR)**\n",
    "\n",
    "`Amazon EMR` provides a managed `Hadoop` framework to process vast amounts of data using AWS for parallel, distributed, elastic execution of data processes and tasks.  \n",
    "`EMR` leverages `S3`, which is their elastic, highly reliable cloud storage product (covered later in the course). \n",
    "  \n",
    "Here is a very short overview (1 min) of EMR:  \n",
    "https://www.youtube.com/watch?v=AM8WZb2Xj2g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, you learned about Spark's architecture, and many options for running a Spark cluster.  \n",
    "\n",
    "The terminology of worker, executor, and driver will come up throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
