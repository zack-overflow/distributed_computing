{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 27, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "Learning Spark 1st Ed., Chapter 3: Programming with RDDs   \n",
    "\n",
    "### OBJECTIVES\n",
    "-  Basics of RDDs including transformations and actions  \n",
    "-  Discuss parallelization concepts  \n",
    "\n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- RDD  \n",
    "- Transformation  \n",
    "- Action  \n",
    "- lazy evaluation  \n",
    "- SparkSession\n",
    "- Directed acyclic graph (Lineage graph)\n",
    "- Set operations  \n",
    "- Pipelining or chaining  \n",
    "- accumulator  \n",
    "- `persist()` and `cache()`  \n",
    "- `parallelize()`  \n",
    "- `collect()` and `take()`  \n",
    "- `map()`, `filter()`, `flatMap()`  \n",
    "- `reduce()`, `fold()`, `aggregate()`  \n",
    "- `count()`, `countByValue()`  \n",
    "- `saveAsTextFile()`, `saveAsSequenceFile()`  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD BASICS\n",
    "\n",
    "An *RDD* is a distributed collection of elements  \n",
    "It is the most basic abstraction in Spark, created at the birth of Spark.\n",
    "\n",
    "All work consists of:  \n",
    "- RDD creation  \n",
    "- RDD transformation  \n",
    "- RDD action (e.g., compute a result)  \n",
    "\n",
    "**When RDDs are Useful**\n",
    "- For unstructured data like documents  \n",
    "- Certain models and applications require them\n",
    "\n",
    "---\n",
    "**ASIDE**  \n",
    "\n",
    "DataFrames are more useful for structured data (e.g., tabular)  \n",
    "We study them later  \n",
    "Under the hood, DataFrames are constructed as rows of RDDs\n",
    "\n",
    "---\n",
    "\n",
    "Spark “magically” handles distributing data and code across cluster, parallelization of operations\n",
    "\n",
    "Spark is \"lazy.\"  \n",
    "It doesn’t actually do any work until it encounters an *action*, for example a `count()`.  \n",
    "\n",
    "**Directed Acyclic Graph (DAG)** \n",
    "\n",
    "Spark creates a logical plan or roadmap to optimize performance of the project.  \n",
    "This is called a *directed acyclic graph* (DAG).  \n",
    "It makes these optimizations without help from the user.  \n",
    "\n",
    "DAG defines steps of program.  \n",
    "Consists of RDD lineages which can be used to re-create RDD in case of job failure.\n",
    "\n",
    "\n",
    "**Example DAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dag.png\" size=100> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When testing/debugging code, it can be helpful to call `count()` to force Spark to evaluate results.  \n",
    "This gives a sense of what breaks and how long things take.  \n",
    "\n",
    "A *transformation* creates a new RDD  \n",
    "\n",
    "An *action* returns a different data type  \n",
    "\n",
    "RDDs are created in two ways:   \n",
    "1. Loading external dataset (`textFile()`, for example)\n",
    "2. Distributing a collection of objects from driver program  \n",
    "\n",
    "for example:\n",
    "```\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "```\n",
    "\n",
    "The `SparkSession` is a single entry point for working with Spark.  \n",
    "It was created in Spark 2.0 to unify and simplify multiple context managers.  \n",
    "For working with RDDs, the `Spark Context` is required. It is an attribute in SparkSession.  \n",
    "The example below illustrates their use. \n",
    "\n",
    "**Example of Transformation: Filter on text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line: 0 text: Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.\n",
      "line: 1 text: Interactive Python Shell\n",
      "line: 2 text: Alternatively, if you prefer Python, you can use the Python shell:\n"
     ]
    }
   ],
   "source": [
    "# import Spark Session from pyspark.sql library\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create SparkSession entry point\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Spark Context is needed for working with RDDs. Extract it from Spark Session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read in a text file\n",
    "lines = sc.textFile(\"README.txt\")\n",
    "\n",
    "# Filter the data by applying a lambda function\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "\n",
    "# Collect the filtered data to the driver\n",
    "py = pythonLines.collect()\n",
    "\n",
    "# For each line of text, print the index and text\n",
    "for i, p in enumerate(py):\n",
    "    print('line: {} text: {}'.format(i,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Operations on RDDs\n",
    "\n",
    "Store or “persist” an RDD by calling  \n",
    "\n",
    "`RDD.persist()` \n",
    "\n",
    "`cache()` is the same as `persist()` with the default storage level\n",
    "\n",
    "`collect()`  \n",
    "Retrieve entire RDD on driver.  \n",
    "Careful w large RDDs, as the results need to fit in memory on single machine!\n",
    "\n",
    "`take()`  \n",
    "Retrieve small number of elements from RDD (user can specify size).  \n",
    "NOTE: values may NOT be in order\n",
    "\n",
    "`first()`  \n",
    "Retrieve first element from RDD  \n",
    "\n",
    "`saveAsTextFile()`, `saveAsSequenceFile()`, `…`  \n",
    "Save contents of RDD as a file. Different function call depending on file storage type.\n",
    "\n",
    "\n",
    "### Some Basic Transformations \n",
    "\n",
    "`map()`  \n",
    "Applies transform to each element in RDD  \n",
    "\n",
    "`flatMap()`  \n",
    "Apply map to produce list of elements in a single list (e.g, tokenize a sentence into words)  \n",
    "\n",
    "`filter()`  \n",
    "Return new RDD with only records meeting condition\n",
    "\n",
    "`parallellize()`  \n",
    "Distribute the data to workers, creating an RDD  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of text processing with `map`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a text file. It happens to be pipe delimited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = sc.textFile(\"pipe_delim_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10|105|-20|mmHg|4', '12|101|-55|mmol|5']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the columns by splitting on pipe delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10', '105', '-20', 'mmHg', '4'], ['12', '101', '-55', 'mmol', '5']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_clean = pipe.map(lambda x: x.split('|'))\n",
    "pipe_clean.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the split converts strings to lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want only a subset of the columns, like the first and third columns.    \n",
    "The first `map` splits the strings, and the second `map` does the subsetting, placing data into tuples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', '-20'), ('12', '-55'), ('8', '-35.7')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_clean_first_two = pipe.map(lambda x: x.split('|')) \\\n",
    "                           .map(lambda x: (x[0], x[2]))\n",
    "\n",
    "pipe_clean_first_two.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', '-20,mmHg,4'), ('12', '-55,mmol,5'), ('8', '-35.7,celsius,2')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_clean_first_and_three_to_end = pipe.map(lambda x: x.split('|')) \\\n",
    "                           .map(lambda x: (x[0], ','.join(x[2:])))\n",
    "\n",
    "pipe_clean_first_and_three_to_end.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity of Transformations\n",
    "\n",
    "Some transformations are simpler than others.\n",
    "\n",
    "For example, `filter()` can operate independently on each data partition, and the results can be combined. This is sometimes called a *narrow transformation*.\n",
    "\n",
    "Computing a median on the data, however, is more complex, since it requires ordering all the data. This means data would need to be shuffled across the cluster, which is an expensive operation.  This is sometimes called a *wide transformation*.\n",
    "\n",
    "It is preferable to keep transformations as simple as possible.\n",
    "\n",
    "**Example of Distributing Data to Workers with `parallelize()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# Show this object is RDD data type\n",
    "print(type(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "\n",
    "Partitions support splitting data into multiple pieces which can be computed in parallel.  \n",
    "This can speed up jobs.\n",
    "\n",
    "Each partition (block of data) lives on a single machine. \n",
    "\n",
    "Every node in a Spark cluster contains one or more partitions.\n",
    "\n",
    "When running locally, Spark partitions data into number of CPU cores on system, or specified value when SparkSession was created.\n",
    "\n",
    "**Example:** Run job locally with 1 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/27 18:23:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('partition_example.com') \\\n",
    "        .master(\"local[1]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# create RDD \n",
    "rdd1  = sc.parallelize((0,25,10,2,4,3,6,8,3,4,0,20,10,2,4,3,6,8,3,4,10,20,10,2,4,3,6,8,3,4))\n",
    "\n",
    "# method to get number of partitions\n",
    "print(rdd1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to change the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Change partitions to 10\n",
    "new_rdd = rdd1.repartition(10)\n",
    "print(new_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save repartitioned data. Notice the number of output files matches number of partitions, but some are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_rdd.saveAsTextFile('repartitioned_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tries to be strategic in how it uses partitions. Later, we will learn how to tune the number of partitions manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = sc.parallelize(['cat','dog','baby'])\n",
    "list2 = sc.parallelize(['giraffe','baby'])\n",
    "\n",
    "# take the union of two RDDs\n",
    "list1.union(list2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this does not filter duplicates  \n",
    "\n",
    "Also notice we can “chain” or “pipeline” commands in sequence  \n",
    "\n",
    "Let’s get the distinct list from the union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1.union(list2).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: `distinct()` is expensive as it requires shuffling all data over the network  \n",
    "\n",
    "Shuffling: the process of redistributing data across partitions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "`reduce()`  \n",
    "Process elements into a new element of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an RDD and sum the integers, two at a time\n",
    "\n",
    "l1 = sc.parallelize([1,2,3,4])\n",
    "sum = l1.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum: {}'.format(sum))\n",
    "print('l1 type: {}'.format(type(l1)))\n",
    "print('sum type: {}'.format(type(sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fold()`  \n",
    "Similar to `reduce()`, includes “zero value” acting as identity  \n",
    "\n",
    "`aggregate()`  \n",
    "Similar to reduce and fold, uses:  \n",
    "1. initial value \n",
    "2. combining function for each worker or node\n",
    "3. combining function to merge results across workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`countbyValue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,3,4])\n",
    "cv = nums.countByValue()\n",
    "\n",
    "print('cv[1]: {}'.format(cv[1]))\n",
    "print('cv[3]: {}'.format(cv[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use `reduce()` to compute the cumulative product of the odd numbers from 1 through 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Print the intersection between these two RDDs, and `collect()` the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(['cat','dog','baby'])\n",
    "rdd2 = sc.parallelize(['giraffe','baby','baby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Print the elements in `rdd1` that are NOT in `rdd2`. The `subtract()` function can be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "l1 = sc.parallelize([val for val in range(1,17,2)])\n",
    "cumprod = l1.reduce(lambda x,y: x*y)\n",
    "cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) \n",
    "\n",
    "rdd1 = sc.parallelize(['cat','dog','baby'])\n",
    "rdd2 = sc.parallelize(['giraffe','baby','baby'])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)\n",
    "\n",
    "rdd1.subtract(rdd2).collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
