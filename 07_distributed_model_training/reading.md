### Reading: Distributed Model Training

---

Note: Some of these readings are relatively short

[Distributed Machine Learning Frameworks and its Benefits](https://www.xenonstack.com/blog/distributed-ml-framework#:~:text=In%20distributed%20machine%20learning%2C%20model,and%20training%20each%20split%20separately.)

[Distributed training with Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training?view=azureml-api-2)

[Distributed Training: Guide for Data Scientists](https://neptune.ai/blog/distributed-training)

[Distributed model training II: Parameter Server and AllReduce](http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/)

[Distributed Machine Learning and the Parameter Server](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture22.pdf)

Uber Engineering introduced Horovod, an open-source component of Michelangeloâ€™s deep learning toolkit which makes it easier to start and speed up distributed deep learning with TensorFlow.  
[Horovod: fast and easy distributed deep learning in TensorFlow](https://arxiv.org/pdf/1802.05799)
