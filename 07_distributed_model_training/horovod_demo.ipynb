{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecf8680-5306-4cef-a274-aa088e889945",
   "metadata": {},
   "source": [
    "### Horovod on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fe0b6-1b85-4d39-befc-f05691c1b52c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Source: https://horovod.readthedocs.io/en/stable/spark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9950fe-365c-4a3e-bf00-6dc3365e7208",
   "metadata": {},
   "source": [
    "**PURPOSE:**  \n",
    "Demo of distributed model training using Horovod with Spark.\n",
    "\n",
    "The Estimator API abstracts the data processing (from Spark DataFrames to deep learning datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d768c105-de08-40cc-92cd-2c57f1e1cb45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: horovod[spark] in /home/apt4c/.local/lib/python3.7/site-packages (0.28.1)\n",
      "Requirement already satisfied: cloudpickle in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (2.0.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (6.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (23.1)\n",
      "Requirement already satisfied: cffi>=1.4.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (1.15.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (1.21.6)\n",
      "Requirement already satisfied: petastorm>=0.12.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (0.12.1)\n",
      "Requirement already satisfied: pyarrow<11.0,>=0.15.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (10.0.1)\n",
      "Requirement already satisfied: fsspec>=2021.07.0 in /home/apt4c/.local/lib/python3.7/site-packages (from horovod[spark]) (2023.1.0)\n",
      "Requirement already satisfied: pyspark>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from horovod[spark]) (3.3.1)\n",
      "Requirement already satisfied: pycparser in /home/apt4c/.local/lib/python3.7/site-packages (from cffi>=1.4.0->horovod[spark]) (2.21)\n",
      "Requirement already satisfied: dill>=0.2.1 in /home/apt4c/.local/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (0.3.7)\n",
      "Requirement already satisfied: diskcache>=3.0.0 in /home/apt4c/.local/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (5.6.3)\n",
      "Requirement already satisfied: future>=0.10.2 in /home/apt4c/.local/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (1.0.0)\n",
      "Requirement already satisfied: pandas>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (1.3.5)\n",
      "Requirement already satisfied: pyzmq>=14.0.0 in /opt/conda/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (24.0.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from petastorm>=0.12.0->horovod[spark]) (1.16.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark>=2.3.2->horovod[spark]) (0.10.9.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.19.0->petastorm>=0.12.0->horovod[spark]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.19.0->petastorm>=0.12.0->horovod[spark]) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install horovod[spark]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a250f-1161-4737-b94e-3e5c46dfc715",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354fadbb-3030-4083-b7c2-ed1aa6b80075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import horovod.spark.keras as hvd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107eaf3-8cd5-4db4-be31-bd752eef9d3c",
   "metadata": {},
   "source": [
    "Import data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2e0993-ef1f-4e2f-a08c-b0d81bf0fcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/sfs/gpfs/tardis/home/apt4c/distributed_computing/04_mllib_intro_and_supervised_learning/'\n",
    "DATA_FILENAME = 'wisc_breast_cancer_w_fields.csv'\n",
    "DATA_FILEPATH = os.path.join(DATA_DIR, DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1e71a7-1a7a-4033-b5c1-6538f1188097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/24 18:42:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c47486f-aa3e-4ab7-8d9f-598f9da55053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(DATA_FILEPATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5831a6c5-724b-4fd8-809e-caf5f4963094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diag_ind = when(col(\"diagnosis\") == 'M', 1).otherwise(0)\n",
    "df = df.withColumn(\"y\", diag_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5daa3567-7e23-40d4-9066-c183dbbe99aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.6, 0.4], seed = 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4737a50-48a8-429f-bce9-06ebf8821045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\"], outputCol=\"features\")\n",
    "train_df = assembler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8414a4-2200-4539-bcdd-1edd7b9a01c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/24 18:43:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+---+-------------+\n",
      "|  y|     features|\n",
      "+---+-------------+\n",
      "|  1|[15.46,19.48]|\n",
      "|  0|[12.89,13.12]|\n",
      "|  0| [14.96,19.1]|\n",
      "|  1|[13.17,18.66]|\n",
      "|  0|[12.18,17.84]|\n",
      "|  1|[22.27,19.67]|\n",
      "|  1|[18.66,17.12]|\n",
      "|  0|[11.15,13.08]|\n",
      "|  0|  [10.8,9.71]|\n",
      "|  1|[13.43,19.63]|\n",
      "+---+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.select('y', 'features').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328d2bd-92d5-4c44-9f34-b845c497d6da",
   "metadata": {},
   "source": [
    "Set up neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71245fed-14fe-4dc6-ad2c-68f5b5a6f612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, activation = 'tanh', input_dim=2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807135de-f530-40aa-ace3-f58198b6cbea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: unscaled learning rate\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss = 'binary_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508911f7-bf5f-4e41-bc15-1a26794bf696",
   "metadata": {},
   "source": [
    "Set up intermediate storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "810b9c57-8894-4e96-be28-fafce59e8394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from horovod.spark.common.store import Store\n",
    "\n",
    "store = Store.create('/tmp/horovod/experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538aa992-b9d5-4106-bdeb-ce79473c27a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_estimator = hvd.KerasEstimator(\n",
    "    num_proc=2,\n",
    "    model=model,\n",
    "    store=store,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    feature_cols=['features'],\n",
    "    label_cols=['y'],\n",
    "    batch_size=32,\n",
    "    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6c588-c666-4dae-aec5-bb72f804a8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partitions=20\n",
      "writing dataframes\n",
      "train_data_path=file:///tmp/horovod/experiment/intermediate_train_data.0\n",
      "val_data_path=file:///tmp/horovod/experiment/intermediate_val_data.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_partitions=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rows=354\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/common/util.py:495: FutureWarning: 'ParquetDataset.schema' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.schema' attribute instead (which will return an Arrow schema instead of a Parquet schema).\n",
      "  train_data_schema = train_data.schema.to_arrow_schema()\n",
      "/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/common/util.py:405: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  for piece in dataset.pieces:\n",
      "/home/apt4c/.local/lib/python3.7/site-packages/horovod/spark/common/util.py:513: FutureWarning: The 'field_by_name' method is deprecated, use 'field' instead\n",
      "  metadata, avg_row_size = make_metadata_dictionary(train_data_schema)\n",
      "2024-10-24 14:43:32.119995: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-10-24 14:43:32.192507: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000010000 Hz\n",
      "2024-10-24 14:43:32.192829: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d21200bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-24 14:43:32.193485: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "[Stage 6:>                                                          (0 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "keras_model = keras_estimator.fit(train_df) \\\n",
    "    .setOutputCols(['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b4956-54b0-4ef5-8e49-1d7775a62321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_df = keras_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13250d3b-9b49-491c-bcc7-55cc2b483054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
